@article{Whang_datacollection,
author = {Whang, Steven Euijong and Roh, Yuji and Song, Hwanjun and Lee, Jae-Gil},
title = {Data collection and quality challenges in deep learning: a data-centric AI perspective},
year = {2023},
issue_date = {Jul 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-022-00775-9},
doi = {10.1007/s00778-022-00775-9},
journal = {The VLDB Journal},
month = jan,
pages = {791–813},
numpages = {23},}




@Article{GAN_artificial_data,
AUTHOR = {Villalonga, Gabriel and Van de Weijer, Joost and López, Antonio M.},
TITLE = {Recognizing New Classes with Synthetic Data in the Loop: Application to Traffic Sign Recognition},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {583},
URL = {https://www.mdpi.com/1424-8220/20/3/583},
PubMedID = {31973078},
ISSN = {1424-8220},
ABSTRACT = {On-board vision systems may need to increase the number of classes that can be recognized in a relatively short period. For instance, a traffic sign recognition system may suddenly be required to recognize new signs. Since collecting and annotating samples of such new classes may need more time than we wish, especially for uncommon signs, we propose a method to generate these samples by combining synthetic images and Generative Adversarial Network (GAN) technology. In particular, the GAN is trained on synthetic and real-world samples from known classes to perform synthetic-to-real domain adaptation, but applied to synthetic samples of the new classes. Using the Tsinghua dataset with a synthetic counterpart, SYNTHIA-TS, we have run an extensive set of experiments. The results show that the proposed method is indeed effective, provided that we use a proper Convolutional Neural Network (CNN) to perform the traffic sign recognition (classification) task as well as a proper GAN to transform the synthetic images. Here, a ResNet101-based classifier and domain adaptation based on CycleGAN performed extremely well for a ratio ∼ 1 / 4 for new/known classes; even for more challenging ratios such as ∼ 4 / 1 , the results are also very positive.},
DOI = {10.3390/s20030583}
}



@Article{blender_paper,
AUTHOR = {Soans, Rahul and Fukumizu, Yohei},
TITLE = {Custom Anchorless Object Detection Model for 3D Synthetic Traffic Sign Board Dataset with Depth Estimation and Text Character Extraction},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {14},
ARTICLE-NUMBER = {6352},
URL = {https://www.mdpi.com/2076-3417/14/14/6352},
ISSN = {2076-3417},
ABSTRACT = {This paper introduces an anchorless deep learning model designed for efficient analysis and processing of large-scale 3D synthetic traffic sign board datasets. With an ever-increasing emphasis on autonomous driving systems and their reliance on precise environmental perception, the ability to accurately interpret traffic sign information is crucial. Our model seamlessly integrates object detection, depth estimation, deformable parts, and text character extraction functionalities, facilitating a comprehensive understanding of road signs in simulated environments that mimic the real world. The dataset used has a large number of artificially generated traffic signs for 183 different classes. The signs include place names in Japanese and English, expressway names in Japanese and English, distances and motorway numbers, and direction arrow marks with different lighting, occlusion, viewing angles, camera distortion, day and night cycles, and bad weather like rain, snow, and fog. This was done so that the model could be tested thoroughly in a wide range of difficult conditions. We developed a convolutional neural network with a modified lightweight hourglass backbone using depthwise spatial and pointwise convolutions, along with spatial and channel attention modules that produce resilient feature maps. We conducted experiments to benchmark our model against the baseline model, showing improved accuracy and efficiency in both depth estimation and text extraction tasks, crucial for real-time applications in autonomous navigation systems. With its model efficiency and partwise decoded predictions, along with Optical Character Recognition (OCR), our approach suggests its potential as a valuable tool for developers of Advanced Driver-Assistance Systems (ADAS), Autonomous Vehicle (AV) technologies, and transportation safety applications, ensuring reliable navigation solutions.},
DOI = {10.3390/app14146352}
}
